{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objectives\n- Intro to PyTorch\n- In-depth understanding of transformers\n- Decoder-only transformer network\n\nHeavy credit to Andrej Karpathy and this video: https://youtu.be/kCc8FmEb1nY although I made many changes.\nSome highlights:\n- Hyperparameters and model more closely following the original paper. (See comments with double quotes for references to the original paper.)\n- Utilizing multiple GPUs\n- Use datasets/dataloaders instead of homegrown batching\n- Label smoothing\n- Better corpus for training :)\n\nUsing the paper as-is results in heavy overfitting (at least with this training data). Further extension would be to investigate how to reduce the overfitting (increase dropout, add it in more places, etc).","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nimport os\nimport time\nfrom torch.nn import functional as F\n\n# Define hyperparameters\nval_split = 0.1\nbatch_size = 64 # how many independent sequences will we process in parallel\nblock_size = 256 # maximum context length, in characters\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 2e-4 # Implementation of varying learning rate is in the training loop\neval_iters = 200\nd_model = 384 # \"d_model = 512\"\nn_head = 6 # \"In this work we employ h = 8\"\nn_layer = 6 # \"a stack of N = 6 identical layers\"\nd_k = d_model // n_head # \"For each of these we use d_k = d_v = d_model/h = 64\" aka head size\nd_ff = 4 * d_model\ndropout = 0.1\nsmoothing = 0.1\nwarmup_steps=4000","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:40:55.333245Z","iopub.execute_input":"2023-08-03T01:40:55.333661Z","iopub.status.idle":"2023-08-03T01:40:59.271151Z","shell.execute_reply.started":"2023-08-03T01:40:55.333629Z","shell.execute_reply":"2023-08-03T01:40:59.270121Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Use multiple GPUs\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:40:59.273230Z","iopub.execute_input":"2023-08-03T01:40:59.273867Z","iopub.status.idle":"2023-08-03T01:40:59.349901Z","shell.execute_reply.started":"2023-08-03T01:40:59.273829Z","shell.execute_reply":"2023-08-03T01:40:59.348746Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Download and Preprocess Data\n\nHere, we download Monty Python and the Holy Grail from NLTK.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('webtext')\nfrom nltk.corpus import webtext\n\n# Get unique characters from text\ntext = webtext.raw('grail.txt')\n\" \".join(text.split()[:45])","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:40:59.351897Z","iopub.execute_input":"2023-08-03T01:40:59.352344Z","iopub.status.idle":"2023-08-03T01:41:00.594154Z","shell.execute_reply.started":"2023-08-03T01:40:59.352307Z","shell.execute_reply":"2023-08-03T01:41:00.593102Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package webtext to /usr/share/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there! [clop clop clop] SOLDIER #1: Halt! Who goes there? ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot. King of the Britons, defeator of the Saxons, sovereign of all England!'"},"metadata":{}}]},{"cell_type":"markdown","source":"Get all unique characters used in the text - this is our vocabulary size.","metadata":{}},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(\"Chars: \" + str(chars))\nprint(\"Vocab size: \" + str(vocab_size))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:00.597473Z","iopub.execute_input":"2023-08-03T01:41:00.598169Z","iopub.status.idle":"2023-08-03T01:41:00.607987Z","shell.execute_reply.started":"2023-08-03T01:41:00.598132Z","shell.execute_reply":"2023-08-03T01:41:00.606751Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Chars: ['\\n', ' ', '!', '#', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nVocab size: 76\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:00.609511Z","iopub.execute_input":"2023-08-03T01:41:00.610718Z","iopub.status.idle":"2023-08-03T01:41:00.620692Z","shell.execute_reply.started":"2023-08-03T01:41:00.610674Z","shell.execute_reply":"2023-08-03T01:41:00.619585Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomWebtextDataset(Dataset):  \n    def __init__(self, raw, block_size):\n        self.encoded_text = encode(raw)\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.encoded_text) - self.block_size\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.encoded_text[idx:idx + self.block_size], dtype=torch.long).to(device)\n        y = torch.tensor(self.encoded_text[idx + 1:idx + self.block_size + 1], dtype=torch.long).to(device)\n        return x, y\n","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:00.624350Z","iopub.execute_input":"2023-08-03T01:41:00.624634Z","iopub.status.idle":"2023-08-03T01:41:00.633519Z","shell.execute_reply.started":"2023-08-03T01:41:00.624609Z","shell.execute_reply":"2023-08-03T01:41:00.632484Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"split_ind = int(len(text)*val_split)\n\ntrain = CustomWebtextDataset(webtext.raw('grail.txt')[:-split_ind], block_size)\nval = CustomWebtextDataset(webtext.raw('grail.txt')[-split_ind:], block_size)\n\ntrain_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\nval_dataloader = DataLoader(val, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:00.634924Z","iopub.execute_input":"2023-08-03T01:41:00.635363Z","iopub.status.idle":"2023-08-03T01:41:00.654662Z","shell.execute_reply.started":"2023-08-03T01:41:00.635328Z","shell.execute_reply":"2023-08-03T01:41:00.653545Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Example x, y\ni, xy = next(enumerate(train_dataloader))\n(x, y) = xy\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:00.656473Z","iopub.execute_input":"2023-08-03T01:41:00.657163Z","iopub.status.idle":"2023-08-03T01:41:04.229724Z","shell.execute_reply.started":"2023-08-03T01:41:00.657094Z","shell.execute_reply":"2023-08-03T01:41:04.228668Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.Size([64, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utility functions","metadata":{}},{"cell_type":"code","source":"# Function to estimate the loss of the model in its current state.\n@torch.no_grad() # good practice, tells pytorch that we won't call backprop on this\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros((eval_iters))\n        for i in range(eval_iters):\n            if split == 'train':\n                j, xbyb = next(enumerate(train_dataloader))\n            else:\n                j, xbyb = next(enumerate(val_dataloader))\n            x, y = xbyb\n            logits, loss = model(x, y)\n            losses[i] = loss.mean().item()\n        out[split] = losses.mean()\n    generate_example(model, 500) # See how the model is doing\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.231236Z","iopub.execute_input":"2023-08-03T01:41:04.232258Z","iopub.status.idle":"2023-08-03T01:41:04.240903Z","shell.execute_reply.started":"2023-08-03T01:41:04.232220Z","shell.execute_reply":"2023-08-03T01:41:04.240075Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def generate_example(model, new_tokens):\n    # generate from the model\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n    print(decode(model.module.generate(context, max_new_tokens=new_tokens)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.244958Z","iopub.execute_input":"2023-08-03T01:41:04.245789Z","iopub.status.idle":"2023-08-03T01:41:04.264454Z","shell.execute_reply.started":"2023-08-03T01:41:04.245752Z","shell.execute_reply":"2023-08-03T01:41:04.263314Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Classes","metadata":{}},{"cell_type":"code","source":"class SelfAttentionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = nn.Linear(d_model, d_k, bias=False)\n        self.query = nn.Linear(d_model, d_k, bias=False)\n        self.value = nn.Linear(d_model, d_k, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(p=dropout)\n    def forward(self, x, d_k):\n        B, T, C = x.shape # Batch size, time dimension, num channels\n        \n        # \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n        # The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the \n        # corresponding key.\"\n        \n        # Self attention: keys/queries/values all come from the same source (self-attending)\n        # Cross-attention would instead have queries coming from x and keys/values coming from a separate source\n        \n        # Self-attention: different tokens find other tokens more or less interesting (data dependent). Gather information from the past in a data-dependent way.\n        # Every token emits a query and a key vector. Query - what am I looking for? Key - what do I contain? Affinities between tokens are given by dot product \n        # between the key and the query.\n        \n        # Start off with \"Scaled Dot-Product Attention\": Attention(Q, K, V) = softmax(Q@K.T/sqrt(d_k))@V\n        qdotk = self.query(x) @ (self.key(x)).transpose(-2, -1) # (B, T, d_k) @ (B, T, d_k).transpose(-2, 1) -> (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n        \n        # We scale by 1/sqrt(d_k) to make the resulting distribution after softmaxing more diffuse - becomes much closer to a one-hot encoding without this step, which is not useful\n        scaled_dotprod_att = qdotk / (d_k**0.5)\n        \n        # \"We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking... ensures\n        # that the predictions for position i can depend only on the known outputs at positions less than i\"\n        # Because this is self-attention, we need to set illegal values to -inf to prevent positions from attending to subsequent positions.\n        # i.e. information only flows from the previous context to the current timestamp, and cannot get any information about the future because we're about to predict the future.\n        scaled_dotprod_att = scaled_dotprod_att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        \n        # Now we can softmax and dot product with v.\n        softmax_att = F.softmax(scaled_dotprod_att, dim=-1)\n        softmax_att = self.dropout(softmax_att)\n        v = self.value(x)\n        return softmax_att @ v\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.266160Z","iopub.execute_input":"2023-08-03T01:41:04.266846Z","iopub.status.idle":"2023-08-03T01:41:04.286890Z","shell.execute_reply.started":"2023-08-03T01:41:04.266793Z","shell.execute_reply":"2023-08-03T01:41:04.285810Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MultiSelfAttentionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = nn.ModuleList([SelfAttentionHead() for _ in range(n_head)])\n        self.proj = nn.Linear(d_k * n_head, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n    \n    def forward(self, x): # input (B, T, C)\n        # MultiHead(Q,K,V) = Concat(head1,...,headh)W^O where headi = Attention(QW_i^Q, KW_i^K, VW_i^V)\n        out = torch.cat([h(x=x, d_k=d_k) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.291443Z","iopub.execute_input":"2023-08-03T01:41:04.293651Z","iopub.status.idle":"2023-08-03T01:41:04.305517Z","shell.execute_reply.started":"2023-08-03T01:41:04.293616Z","shell.execute_reply":"2023-08-03T01:41:04.304435Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # FFN(x) = max(0,wW1 + b1)W2 + b2\n        # \"The dimensionality of input and output is d_model = 512, and the inner-layer has dimensionality d_ff = 2048.\"\n        self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(p=dropout), # \"We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\"\n        )\n    \n    def forward(self, x):\n        return self.ff(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.310460Z","iopub.execute_input":"2023-08-03T01:41:04.313454Z","iopub.status.idle":"2023-08-03T01:41:04.322574Z","shell.execute_reply.started":"2023-08-03T01:41:04.313408Z","shell.execute_reply":"2023-08-03T01:41:04.321375Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.msah = MultiSelfAttentionHead()\n        self.ff = FeedForward()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n    def forward(self, x): # input (B, T, C)\n        # \"That is, the output of each sub-layer is LayerNorm(x + Sublayer(x))\"\n        # This was explained as \"FFN (MLP) is just 'thinking' on the previous 'communication' (attention). Repeated decoder blocks are just interspersing thinking w/ communicating.\"\n        # Layer norm after skip connections as shown in the figure\n        x = self.ln1(x + self.msah(x))\n        x = self.ln2(x + self.ff(x))\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.328841Z","iopub.execute_input":"2023-08-03T01:41:04.331735Z","iopub.status.idle":"2023-08-03T01:41:04.341288Z","shell.execute_reply.started":"2023-08-03T01:41:04.331700Z","shell.execute_reply":"2023-08-03T01:41:04.340128Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, d_model) # (vocab_size, C)\n        # From the paper, creating a position embedding table yields \"nearly identical results\" to using the sinusoidal positional encoding\n        self.position_embedding_table = nn.Embedding(block_size, d_model) # (block_size, C)\n        self.dropout = nn.Dropout(p=dropout)\n        self.blocks = nn.Sequential(*[TransformerBlock() for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # idx and targets are both (B,T) tensor of integers\n        \n        # Convert tokens to embedding vectors, each with length d_model\n        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n        # Add positional data via embeddings\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n\n        x = tok_emb + pos_emb # (B, T, C)\n        x = self.dropout(x) # \"n addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\"\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets, label_smoothing=smoothing)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.346346Z","iopub.execute_input":"2023-08-03T01:41:04.349468Z","iopub.status.idle":"2023-08-03T01:41:04.372618Z","shell.execute_reply.started":"2023-08-03T01:41:04.349433Z","shell.execute_reply":"2023-08-03T01:41:04.371564Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Train loop\n\nmodel = Transformer()\nmodel = nn.DataParallel(model)\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-09)\n\nstart = time.time()\nfor iter in range(max_iters):\n    # Vary the learning rate as in the paper - increase learning linearly for the first warmup_steps training steps, \n    # and decreasing it thereafter proportionally to the inverse square root of the step number.\n    for g in optimizer.param_groups:\n        g['lr'] = (d_model**-0.5)*min((iter+1)**-0.5, (iter+1)*(warmup_steps**-1.5))\n\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        now = time.time()\n        print(f\"After {(now - start)} seconds: step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    # sample a batch of data\n    #xb, yb = get_batch('train')\n    i, xbyb = next(enumerate(train_dataloader))\n    xb, yb = xbyb\n    \n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.mean().backward()\n    optimizer.step()\n    \ngenerate_example(model, 500)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:41:04.378290Z","iopub.execute_input":"2023-08-03T01:41:04.382801Z","iopub.status.idle":"2023-08-03T02:17:27.049031Z","shell.execute_reply.started":"2023-08-03T01:41:04.382766Z","shell.execute_reply":"2023-08-03T02:17:27.047914Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"\nR# .S?13k[EIH\nnL;y2J;(RB;KQeNo))AuwClKJ),.6)Fb1z'T0-2(68vyv8:s(7Y#).l9o?7S\nb\nZYQB\nim\nlT]wxqilOb872Nq fSFq)SpbPKYpTRavQYRy'TWEN:E-P#SDI:jI.S8gW9FEyxeS(Wg:SFSFITYx pRGDQb1U9,YV\n(VH5VI:pxKT8,QMCJuljpH2SNt'[ChiLg?,(:!REI#M9mVsrAUmzdgpW-c8.2z'WS5p(.:9#tx#iCzC.8I?]'Qun73Rz0pSCRRIyLr\nAe[qj hJ1z4mAve7sLkAhyF1owAN8'J6FRCI90J'0:i4O.#KR?4rMl,YINKGrx(q\nSa]a0e0t4JIx)KI'IMmzBAEpF5rLgKy9RYIsP92yR]qBb\nN\n,RJz!q0ZJce['Fd3HWeG[sg;vpZS'b\n#Y[IVemYtgAJ'J)O 2nt0D]ar]eS[RiA(Vt.AN[RIFhU#3yq[\nviqk2j9k,N'icjbv[DRp!U1pJMC7\nAfter 59.64755892753601 seconds: step 0: train loss 4.4978, val loss 4.5022\n\nKNGH6AHA?\nAHTHUNCR: URTHUR: Bly ba, or. ShZo..   Yole yowhing s pee !\nARTHEREm NCEN KNCEVIS: bRopp s2: d s o, ayo! Conery, yis...p  Bldem.. BGUerju's ahr l cicjStoresquacoue OTay Umain! \nON WDERT BE:3:Dis bet'sm Hes, VERch, Sor9 oncou nohut!\ncep!   Wh!\nARTECERT: INon!.\nD:, utcioap [m,RENell, yof BERTI'thit c0jur ck CHSOTE: andat, bo, c] Zoondin.5: cahe . rg] w Sanyot!  Caghmbe Ife mst, yoURERve ad meat t, Founomverrd ou y.\nDEN: st Nouis8: m t7uShe, ug 6th! frtPre..Qo.zyste CEARTHofat T- Heste ou\nAfter 265.5240020751953 seconds: step 500: train loss 2.6381, val loss 2.8427\n\n6Onp musep flotr thins ra manglem!\nSCEIJ 7Eing] Hup Hesel.\nd [ingingh] He's the lapEAh.  Sp)p'sit!\nFRENCH niver a the king ight!  Ve sfG\nKNIGHTS: Right (F0r!  Rouge..  Noh. [pbom] \nHEADGUAD KS: [thumpsic cki, pliek] BLACKSZng ozd ose for the f--p-\nARTHUR: That Inoug, have, ir int.  What's the laYFATHER: Sh?\nFATHER: But and arige you dNo,My.  [Knight.\nFATHER:Ni!\nGUARD #2: Oh, whe you manted a whre10:J: SiI t..  And----\nFRENC] OJ: Ah, now lick se.\nGUARD KFTNIGHT: Aaaaaaaaaaah?\nZOTOOT: Nu!  ;nd cha\nAfter 477.64388370513916 seconds: step 1000: train loss 1.9015, val loss 2.3531\n\nout; Launcelot, ..  [couny,xping] ..M: Hello.  ha pleas least the Grail?\nPIGLET:Mo. Yes.  CRight!\nGALAHAD: Loook!\nPRT:God!  Con!\nGALAHAD: Zoood, shall0we to my a! kllbjooAD: Just you killceweders!\nM12nercead!\nLAUNCELOT: Oh, you do, eKnights.\nHEAD #PERSCONER(: [Trump1ble.] ...ive [whPic scrab]\nFRENCONPR: Ican. [clap cla?\nFRENCHPck] \nHCARACTEicer Chastl[th] [twQJyay paunk] \nBEDEVERE: And that?9RE [wh #2] [clund] [rewr!  Pi0le must h--\nck] Ok, thatquCK 2we cout for your do5r! [clang] \nCROWD: A witc\nAfter 689.3557376861572 seconds: step 1500: train loss 1.3530, val loss 2.3754\n\n! Hlo.  Halt!  Hold be it!#1\nLAUNCELOT: Hthat!  Hoa!E2: [scrashquemFLAUNCELOTeHAo.  It's not off thVDLord!\nARTHUR: Yes, back!\nLAUNCELuES: [cry#1Gar]BLAGER #2: Chiourd\nLACK KNIGHT: Chicken! [trumpeus]e \nARTHUR: Concox6It !  Corde!  Concorde!  RThe \nSCENE 114:\nARTHUR: H(SIS\nLAF1: [3tGack] \nbARTHUR: ErOn the he dHo. [aARTHUR2: Eh.  Shh?  !\nCRONE: Uh.EHERBERAD: NZ7:\nAR] \n#9ARTHUR: Jes; \nyingRng3Yht[clOGER: qYou kit5UKNIGHTS: Well, you se, then.  I'm aFr, strabbot.  Come on, Potsy.ae!  Iea9: Quiete k\nAfter 901.3237698078156 seconds: step 2000: train loss 0.9814, val loss 2.6707\n\nTIMYp a FRENCH GUARD: All right.  Yo1: What, but I don't tZot)\nVILLAGER #1: got to he, bave bry suOeght cSCENDRsATtqG: [clquack] \nBEDBEVERE: Are : you by Kurque5esK 8#2: Ooh: the youing sheutt als word ork?\nARTHUR: You know6: all the Wat, are you go on to quest for t7he kee[clang] [squeak] \nCROWD: A wiQ: I iddefNiot!  xpence] W! just  EeMake# 6NKS [chanting] Sco?0: [boom Zk] [thud thud] [fingingjf] me ewiny megts Roing #1ble)\nMAYNARWf5: Ye!\nNA4: [clange 9HeeMge sure tB)eak [squeay ndeak\nCROWD: A\nAfter 1112.5203204154968 seconds: step 2500: train loss 0.9061, val loss 2.6661\n\nVILLAGER #1: BuLej-- ust ,ZOh, han[tw3oo1?5]  Oh] fKing eB(Yes, I h3: have br?  uhht a !  ] Right.  You are dFari3a--\nr6PRINCE GO: Oh, s\nLAUNCELOT:VE c#2: A wictch!  A witch!  ch!  A witchBES !  o, wMomVike the bfor of the Maynarg4PBUERT: [chFor)EDEVEREkx:otd] \nARTHUR: Please, pDogobHcMAYt he mmore liUng fortherERBERT: OoohghxDk3LAUNCELOT:)!1: Ao.  It'll !gkYouseTp oW8: That's !2-?\nZOOT: Gfood!  HerpjNoing.  Stop it!  Who are th#2: You are the gFrenc, ocking thw\noughmnest in the caK#1: AntDUnWil\nAfter 1324.48659324646 seconds: step 3000: train loss 0.8724, val loss 2.6299\n\nMONKS: [chanting] Pie I4invCANDON CHARD: Hp-- Fo?\nOTMAN[op pjVERS: YelaxY u(: ein land.  [boom]NournSTy SY-CENE 2: [clop clop clop] [w: [boom] [angels sing] \nGOD: Arthur!  Arthur, Kinng of the Britons!  Oh, don't grovel! [sinY: Oh, 4mLo!\nGOD: My )tP)Wt.worQuickGLAHAD: Caft your Josest got6: Oh, xWecust that is a shickeP?\nGYNARD Wehare Gof UARD: , course!  Joseph oLAUNCELOT: Look!  sJust ke: icke]witNrd here for-s\nBEDEVERE: And therefore?\nVILLAGMY: The nWell youxp ca3Zoot?\nNARRATok, what is got b\nAfter 1535.6282002925873 seconds: step 3500: train loss 0.8509, val loss 2.6068\n\nROBIN: SplenidZriblet'.\nNARRATOR: The TalaG'awa4may in the BNold L9:ord! [WhED: like boom o5SUy #1: Zoot!\n?\nLAUNCELOT: 'Ming RiGden!\nARTHUL: Sir leadTn78: [Zoo] \nGALAHAD: Oh, ha ha ha!  Hee; ha ha..\nARTHUR: Where dPrincess KiQuits of the RounY The 8: Dra: PrincPry,  here wiWh mring in the doctors immediaried at .\nLAUNCELOT: No, we'v[2tmusic] 5: ...  stagain!  zo your reDMoning wit( vayhe romunht. ly Jy.suQuiy.  Come on, 8? [King ArthurUCEEN1d no-FarTlqu: iteven.E)  1MorhKfiMONI: [zing lizth is ,\nAfter 1746.6857125759125 seconds: step 4000: train loss 0.8327, val loss 2.6188\n\nCONCORDE: RigActormankhSUER: [whi3: Oh, caNoiK4: Oh, hanhike Knights Roh7; Yes.  It'Grail.  Yes.  Mean ei'?\nROBIN: Would he is upQuHEAD KNIGHT: l2quiYPing it you!\nROBIN: Ohh!\nARTHUR:Do on[thonk] \nSCENE 17:\nGUESTS: [crying] \nFHEAD5: Hey!\nkingUNCes veryM'.\nNARRATOR, be For, for Enice the c!\ngtUARMARD #1:Oh, , not biscuits.BEDEVERE: Uh, but can you not also magest mine.\nDENNIS:o hat's will ittl Qoing itto aEurwhingUve)(vest xpl[boog2h Yy] [O\nea4C1: [Vick] \nVOFICE: Picture for S, CEHaynter9ess08: Th\nAfter 1957.650313615799 seconds: step 4500: train loss 0.8141, val loss 2.6683\n\nMAYNARDh, don't what I'm thave prond the moZOOT: Yes, you Uh, she's not of you tell whums where we could man has enter.\nFQENCH GUARD: 8k!  Oh, yeah.  True.  must point!\nGUVILLS: Tell se's not get Oooh.\nGUESTS: The 4en 5aint ASYay, spenking the )V!  'Tim dead we're walking I  thought yxpeople woul9.  Beazo1Son't brush my teeth.\nple lasDINGO: Yes!  O6: Oh, yes.  Let him handle us easily.\nGIRLS: Yes he hadm--\nARTHUR: Pl'se, what the lea4IRTHlease, E.  I am ...  an thD That same the ...\nBE5: [9weet \nAfter 2168.180025100708 seconds: step 4999: train loss 0.8029, val loss 2.7532\n\nSOLDGER #1: Wel) heVld u3ze founzder the BritoQy wherring of7I#1: MornHind!\n: Y(FATHER: I9: It could Ve)kly n#dIh, h, s] witcTronuSAF\n?\nHERBE-T: Rjust he4J: bEnQKHP--quiY\nFATHER:LomY: At  6o.Hye'!\nBEDEVERE: You5s fooling!\nARTHUR: GoodO1: 9ick9: LeVICto us shep4ver-- [kiEREj: mzgiU)VER: justDy Zoot.  Oh, wky now, you!VGEir!HERBERT: ): I'm reF#1: yoou gh the fo2nm] \nFATHERquite) \nGUARD #1: The )oT)ust How?\nFATHEg's Will 2;RBERwherequwritch!#[knight.\nWOMAN: We No don!  I Prov9;y for theJqusZOOT: Te\n","output_type":"stream"}]}]}